{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 08: Data streams\n",
    "\n",
    "In this session we will take a large corpus of documents and compute some statistics using data streams methods.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <font color=\"blue\">Bernat Quintilla Castell√≥n</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">bernat.quintilla01@estudiant.upf.edu</font>\n",
    "\n",
    "Date: <font color=\"blue\">The current date here</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "import gzip\n",
    "import random\n",
    "import statistics\n",
    "import secrets\n",
    "import re\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset and how to iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input file contain lines of dialogue of a set of movies from the [Movie Dialog Corpus](https://www.kaggle.com/datasets/Cornell-University/movie-dialog-corpus). We will use the file `movie_lines.tsv` which contains the text of the dialogue, about 3 million words in about 300,000 lines of dialogue.\n",
    "\n",
    "During this practice, **we will never load this file in memory.**\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "INPUT_FILE = \"movie_lines.tsv.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `read_by_words` is a [generator](https://wiki.python.org/moin/Generators), that is, a function that behaves as an iterator. This is a common pattern used in stream processing, and in Python is implemented with the `yield` keyword, instead of `return`.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "# Producer in Python that reads a filename by words\n",
    "def read_by_words(filename, max_words=-1, report_every=-1):\n",
    "    \n",
    "    # Open the input file\n",
    "    with gzip.open(INPUT_FILE, \"rt\", encoding='utf8') as file:\n",
    "        \n",
    "        # Initialize counter of words to stop at max_words\n",
    "        counter = 0\n",
    "    \n",
    "        # Regular expression to identify words having 3 letters or more and beginning with a-z\n",
    "        word_expr = re.compile('^[a-z]{2,}$', re.IGNORECASE)\n",
    "\n",
    "        # Iterate through lines in the file\n",
    "        for line in file:\n",
    "            \n",
    "            elements = line.split(\"\\t\")\n",
    "            \n",
    "            text = \"\"\n",
    "            if len(elements) >= 5:\n",
    "                text = elements[4].strip()\n",
    "                                        \n",
    "            if counter > max_words and max_words != -1:\n",
    "                break\n",
    "                \n",
    "            for word in nltk.word_tokenize(text):\n",
    "                          \n",
    "                if word_expr.match(word):\n",
    "                    counter += 1\n",
    "                    \n",
    "                    # Report\n",
    "                    if (report_every != -1) and (counter % report_every == 0):\n",
    "                        if max_words == -1:\n",
    "                            print(\"- Read %d words so far\" % (counter))\n",
    "                        else:\n",
    "                            print(\"- Read %d/%d words so far\" % (counter, max_words))\n",
    "\n",
    "                    # Produce the word in lowercase\n",
    "                    yield word.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do a first pass over the data. Here we will read only the first 300K words. Try with a larger limit if your computer is fast, with a lower limit if your computer is slow. Find something that makes one pass take about 30 seconds and use it for development.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current word 'nakedness'\n",
      "Current word 'women'\n",
      "Current word 'die'\n",
      "Current word 'simon'\n",
      "Current word 'your'\n",
      "Current word 'aware'\n",
      "Current word 'that'\n",
      "Current word 'said'\n",
      "- Read 100000/300000 words so far\n",
      "Current word 'do'\n",
      "Current word 'before'\n",
      "Current word 'should'\n",
      "Current word 'did'\n",
      "Current word 'my'\n",
      "Current word 'out'\n",
      "Current word 'he'\n",
      "Current word 'other'\n",
      "Current word 'feels'\n",
      "Current word 'and'\n",
      "Current word 'whole'\n",
      "Current word 'it'\n",
      "- Read 200000/300000 words so far\n",
      "Current word 'on'\n",
      "Current word 'things'\n",
      "Current word 'grant'\n",
      "Current word 'fuck'\n",
      "Current word 'you'\n",
      "Current word 'gusta'\n",
      "Current word 'but'\n",
      "Current word 'so'\n",
      "- Read 300000/300000 words so far\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "# Iterate through the file\n",
    "for word in read_by_words(INPUT_FILE, max_words=300000, report_every=100000):\n",
    "    # Prints 1/10000 of words\n",
    "    if random.random() < 0.0001:\n",
    "        print(\"Current word '%s'\" % (word)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\berna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this if above gives an error about 'punkt'\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Determine approximately the top-10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of loading the entire dataset in main memory, we will use reservoir sampling to determine approximately the top-10 words.\n",
    "\n",
    "**Reservoir sampling**: In reservoir sampling, if we have a reservoir of size S:\n",
    "\n",
    "* We store the first S elements of the stream\n",
    "* When the n<sup>th</sup> element arrives (let's call it X<sub>n</sub>):\n",
    "   * With probability 1 - s/n, we ignore this element.\n",
    "   * With probability s/n, we:\n",
    "      * Discard a random element from the reservoir\n",
    "      * Add element X<sub>n</sub> to the reservoir (calling *add_to_reservoir*)\n",
    "      \n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `add_reservoir(reservoir, item, max_size)` that adds an item to the reservoir, maintaining its size. If the reservoir is already of size *max_size*, a random item is selected and evicted *before* adding the item. It is important to evict an old item *before* adding the new item. Use the following skeleton:\n",
    "\n",
    "```python\n",
    "def add_to_reservoir(reservoir, item, max_reservoir_size):\n",
    "    # YOUR CODE HERE\n",
    "    assert(len(reservoir) <= max_reservoir_size)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"add_reservoir\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_reservoir(reservoir, item, max_reservoir_size):\n",
    "    # YOUR CODE HERE\n",
    "    if len(reservoir) < max_reservoir_size:\n",
    "        reservoir.append(item)\n",
    "    else:\n",
    "        item_to_remove = random.choice(reservoir)\n",
    "        reservoir.remove(item_to_remove)\n",
    "        reservoir.append(item)\n",
    "    assert(len(reservoir) <= max_reservoir_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to iterate through the file using the reservoir sampling method seen in class. In this function you will decide, for every item, whether to call *add_to_reservoir* or to ignore the item.\n",
    "\n",
    "You can use the following skeleton:\n",
    "\n",
    "```python\n",
    "def reservoir_sampling(filename, reservoir_size, max_words=-1, report_every=-1):\n",
    "    reservoir = []\n",
    "    \n",
    "    words_read = 0\n",
    "    \n",
    "    for word in read_by_words(filename, max_words=max_words, report_every=report_every):\n",
    "    \n",
    "            # YOUR CODE HERE\n",
    "\n",
    "    return (words_read, reservoir)\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"reservoir_sampling\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_sampling(filename, reservoir_size, max_words=-1, report_every=-1):\n",
    "    reservoir = []\n",
    "    words_read = 0\n",
    "\n",
    "    for word in read_by_words(filename, max_words=max_words, report_every=report_every):\n",
    "        words_read += 1\n",
    "\n",
    "        #Reservoir sampling\n",
    "        if len(reservoir) < reservoir_size:\n",
    "            #If reservoir is not full add word directly\n",
    "            reservoir.append(word)\n",
    "        else:\n",
    "            #Reservoir is full\n",
    "            #Probability of replacing an existing word is reservoir_size/words_read\n",
    "            replace_probability = reservoir_size / words_read\n",
    "\n",
    "            if random.random() < replace_probability:\n",
    "                add_to_reservoir(reservoir, word, reservoir_size)\n",
    "\n",
    "    return words_read, reservoir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function using the following code:\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 100000/1000000 words so far\n",
      "- Read 200000/1000000 words so far\n",
      "- Read 300000/1000000 words so far\n",
      "- Read 400000/1000000 words so far\n",
      "- Read 500000/1000000 words so far\n",
      "- Read 600000/1000000 words so far\n",
      "- Read 700000/1000000 words so far\n",
      "- Read 800000/1000000 words so far\n",
      "- Read 900000/1000000 words so far\n",
      "- Read 1000000/1000000 words so far\n",
      "Number of items seen    : 1000023\n",
      "Number of items sampled : 1500\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "reservoir_size = 1500\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, reservoir_size, max_words=1000000, report_every=100000)\n",
    "\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reservoir contains repeated items. You can compute the absolute frequencies of the top 10 using the following code.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 you\n",
      "54 the\n",
      "41 to\n",
      "35 it\n",
      "28 me\n",
      "27 do\n",
      "26 and\n",
      "23 that\n",
      "19 for\n",
      "18 of\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:10]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"%d %s\" % (absolute_frequency, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to compute the 15 most frequent items in the reservoir and their relative frequencies, as percentages.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to print the top items and their relative frequencies</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 you (5.20%)\n",
      "54 the (3.60%)\n",
      "41 to (2.73%)\n",
      "35 it (2.33%)\n",
      "28 me (1.87%)\n",
      "27 do (1.80%)\n",
      "26 and (1.73%)\n",
      "23 that (1.53%)\n",
      "19 for (1.27%)\n",
      "18 of (1.20%)\n",
      "17 we (1.13%)\n",
      "16 on (1.07%)\n",
      "15 is (1.00%)\n",
      "15 he (1.00%)\n",
      "13 what (0.87%)\n"
     ]
    }
   ],
   "source": [
    "#Compute total count of items in the reservoir\n",
    "total_items = len(reservoir)\n",
    "\n",
    "#Compute and print top 15 most frequent items and their relative frequencies\n",
    "top_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:15]\n",
    "\n",
    "for absolute_frequency, word in top_items:\n",
    "    relative_frequency = (absolute_frequency / total_items) * 100\n",
    "    print(\"%d %s (%.2f%%)\" % (absolute_frequency, word, relative_frequency))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see an item C times in the reservoir, you can estimate the item appears *C x dataset_size / reservoir_size* times in the entire dataset (*dataset_size* is the size of the entire dataset). \n",
    "\n",
    "For various sizes of the reservoir, e.g., 50, 100, 500, ..., list the top-5 words and your estimate of their frequency in the entire dataset.\n",
    " \n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Increase the max limit of words so that one pass takes no more than 5 minutes to be completed. Replace this cell with your code to try different reservoir sizes. In each case, print your estimate for the relative and absolute frequency of the words in the entire dataset.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reservoir Size: 50\n",
      "========================================\n",
      "- Read 100000/300000 words so far\n",
      "- Read 200000/300000 words so far\n",
      "- Read 300000/300000 words so far\n",
      "you:\n",
      "  Absolute Frequency: 78\n",
      "  Relative Frequency: 0.03%\n",
      "  Estimated Frequency in Entire Dataset: 468000.00\n",
      "\n",
      "the:\n",
      "  Absolute Frequency: 54\n",
      "  Relative Frequency: 0.02%\n",
      "  Estimated Frequency in Entire Dataset: 324000.00\n",
      "\n",
      "to:\n",
      "  Absolute Frequency: 41\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 246000.00\n",
      "\n",
      "it:\n",
      "  Absolute Frequency: 35\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 210000.00\n",
      "\n",
      "me:\n",
      "  Absolute Frequency: 28\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 168000.00\n",
      "\n",
      "\n",
      "Reservoir Size: 100\n",
      "========================================\n",
      "- Read 100000/300000 words so far\n",
      "- Read 200000/300000 words so far\n",
      "- Read 300000/300000 words so far\n",
      "you:\n",
      "  Absolute Frequency: 78\n",
      "  Relative Frequency: 0.03%\n",
      "  Estimated Frequency in Entire Dataset: 234000.00\n",
      "\n",
      "the:\n",
      "  Absolute Frequency: 54\n",
      "  Relative Frequency: 0.02%\n",
      "  Estimated Frequency in Entire Dataset: 162000.00\n",
      "\n",
      "to:\n",
      "  Absolute Frequency: 41\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 123000.00\n",
      "\n",
      "it:\n",
      "  Absolute Frequency: 35\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 105000.00\n",
      "\n",
      "me:\n",
      "  Absolute Frequency: 28\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 84000.00\n",
      "\n",
      "\n",
      "Reservoir Size: 500\n",
      "========================================\n",
      "- Read 100000/300000 words so far\n",
      "- Read 200000/300000 words so far\n",
      "- Read 300000/300000 words so far\n",
      "you:\n",
      "  Absolute Frequency: 78\n",
      "  Relative Frequency: 0.03%\n",
      "  Estimated Frequency in Entire Dataset: 46800.00\n",
      "\n",
      "the:\n",
      "  Absolute Frequency: 54\n",
      "  Relative Frequency: 0.02%\n",
      "  Estimated Frequency in Entire Dataset: 32400.00\n",
      "\n",
      "to:\n",
      "  Absolute Frequency: 41\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 24600.00\n",
      "\n",
      "it:\n",
      "  Absolute Frequency: 35\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 21000.00\n",
      "\n",
      "me:\n",
      "  Absolute Frequency: 28\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 16800.00\n",
      "\n",
      "\n",
      "Reservoir Size: 1000\n",
      "========================================\n",
      "- Read 100000/300000 words so far\n",
      "- Read 200000/300000 words so far\n",
      "- Read 300000/300000 words so far\n",
      "you:\n",
      "  Absolute Frequency: 78\n",
      "  Relative Frequency: 0.03%\n",
      "  Estimated Frequency in Entire Dataset: 23400.00\n",
      "\n",
      "the:\n",
      "  Absolute Frequency: 54\n",
      "  Relative Frequency: 0.02%\n",
      "  Estimated Frequency in Entire Dataset: 16200.00\n",
      "\n",
      "to:\n",
      "  Absolute Frequency: 41\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 12300.00\n",
      "\n",
      "it:\n",
      "  Absolute Frequency: 35\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 10500.00\n",
      "\n",
      "me:\n",
      "  Absolute Frequency: 28\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 8400.00\n",
      "\n",
      "\n",
      "Reservoir Size: 5000\n",
      "========================================\n",
      "- Read 100000/300000 words so far\n",
      "- Read 200000/300000 words so far\n",
      "- Read 300000/300000 words so far\n",
      "you:\n",
      "  Absolute Frequency: 78\n",
      "  Relative Frequency: 0.03%\n",
      "  Estimated Frequency in Entire Dataset: 4680.00\n",
      "\n",
      "the:\n",
      "  Absolute Frequency: 54\n",
      "  Relative Frequency: 0.02%\n",
      "  Estimated Frequency in Entire Dataset: 3240.00\n",
      "\n",
      "to:\n",
      "  Absolute Frequency: 41\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 2460.00\n",
      "\n",
      "it:\n",
      "  Absolute Frequency: 35\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 2100.00\n",
      "\n",
      "me:\n",
      "  Absolute Frequency: 28\n",
      "  Relative Frequency: 0.01%\n",
      "  Estimated Frequency in Entire Dataset: 1680.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Function to estimate the frequency in the entire dataset\n",
    "def estimate_frequency(absolute_frequency, dataset_size, reservoir_size):\n",
    "    return (absolute_frequency * dataset_size) / reservoir_size\n",
    "\n",
    "reservoir_sizes = [50, 100, 500, 1000, 5000]\n",
    "\n",
    "#Iterate over different reservoir sizes\n",
    "for reservoir_size in reservoir_sizes:\n",
    "    print(f\"\\nReservoir Size: {reservoir_size}\\n{'=' * 40}\")\n",
    "\n",
    "    #Perform reservoir sampling\n",
    "    words_read, reservoir = reservoir_sampling(INPUT_FILE, reservoir_size, max_words=300000, report_every=100000)\n",
    "\n",
    "    #Compute total count of items in entire dataset\n",
    "    total_dataset_size = words_read * (300000 / words_read)\n",
    "\n",
    "    #Compute and print top 5 most frequent items and their estimates\n",
    "    top_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:5]\n",
    "\n",
    "    for absolute_frequency, word in top_items:\n",
    "        relative_frequency = (absolute_frequency / words_read) * 100\n",
    "        estimated_frequency = estimate_frequency(absolute_frequency, total_dataset_size, reservoir_size)\n",
    "\n",
    "        print(f\"{word}:\")\n",
    "        print(f\"  Absolute Frequency: {absolute_frequency}\")\n",
    "        print(f\"  Relative Frequency: {relative_frequency:.2f}%\")\n",
    "        print(f\"  Estimated Frequency in Entire Dataset: {estimated_frequency:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find by trial and error, and include in your report, the minimum reservoir size you need to have somewhat stable results (e.g., the same top-3 words in two consecutive runs of the algorithm).\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Remove the max limit of words and re-run. Replace this cell with a brief commentary indicating what reservoir size you would recommend to use, and your overall conclusions.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determine approximately the distinct number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will estimate the number of distinct words without creating a dictionary or hash table, but instead, we will use the Flajolet-Martin probabilistic counting method.\n",
    "\n",
    "**Flajolet-Martin probabilistic counting**:\n",
    "\n",
    "* For several passes\n",
    "   * Create hash funcion h\n",
    "   * For every element *u* in the stream:\n",
    "      * Compute hash value *h(u)*\n",
    "      * Let *r(u)* be the number of trailing zeroes in *h(u)*\n",
    "      * Maintain *R* as the maximum value of *r(u)* seen so far\n",
    "   * Add *2<sup>R</sup>* as an estimate for the number of distinct elements *u* seen\n",
    "* The final estimate is the average or the median of the estimates found in each pass\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to count trailing zeroes in the binary representation of a number.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def count_trailing_zeroes(number):\n",
    "    count = 0\n",
    "    while number & 1 == 0:\n",
    "        count += 1\n",
    "        number = number >> 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to generate a random hash function. Note this generates a function, so you can do `hash_function = random_hash_function()` and then call `hash_function(x)` to compute the hash value of `x`. \n",
    "\n",
    "We want to make sure each hash is different, so we will create each hash function with a different [salt](https://en.wikipedia.org/wiki/Salt_(cryptography)), which is an additional input that we will take using a good random string generator from the [secrets](https://docs.python.org/3/library/secrets.html) library.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def random_hash_function():\n",
    "    # We use a cryptographically safe generator for the salt of our hash function\n",
    "    salt = secrets.token_bytes(32)\n",
    "    return lambda string: hash(string + str(salt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform *number_of_passes* passes over the file, reading the entire file on each pass (we don't use the reservoir in this part). In each pass, create a new hash function and use it to hash userids. Keep the maximum number of trailing zeroes seen in the hash value of a userid. \n",
    "\n",
    "```python\n",
    "number_of_passes = 5\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    # YOUR_CODE_HERE: read the file and generate an estimate\n",
    "    \n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "```\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to perform the requested number of passes.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Average of estimates: 62259.2\n",
      "* Median  of estimates: 16384.0\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the median of estimates obtained in 3 separate runs of your algorithm; each run should do 10 passes over the file. \n",
    "\n",
    "Increase the numbe of passes to 20 and perform 3 separate runs.\n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Remove the limit of max words, or set to a high number, but notice that you do no need to use more than one hour of computer processing time, and perform the 10 passes. Replace this cell with the results you obtained in each pass, and whether the average or the median seem more appropriate for this probabilistic counting.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELIVER (individually)\n",
    "\n",
    "Remember to read the section on \"delivering your code\" in the [course evaluation guidelines](https://github.com/chatox/data-mining-course/blob/master/upf/upf-evaluation.md).\n",
    "\n",
    "Deliver a zip file containing:\n",
    "\n",
    "* This notebook\n",
    "\n",
    "## Extra points available\n",
    "\n",
    "For more learning and extra points, notice that the number of **distinct** words in a corpus, as a function of the **total** number of words in the corpus, follows an empirical law known as [Heap's Law](https://en.wikipedia.org/wiki/Heaps%27_law).\n",
    "\n",
    "Repeat the probabilistic counting experiment for various values of `max_word` and plot the total number of words read versus the number of distinct words (remember to label axes). Check if it follows Heap's law.\n",
    "\n",
    "Please note that using probabilistic counting means a substantial amount of noise will be introduced and perhaps the Heap's law will not be clear in your plot.\n",
    "\n",
    "**Note:** if you go for the extra points, add ``<font size=\"+2\" color=\"blue\">Additional results: Heap's law</font>`` at the top of your notebook. \n",
    "\n",
    "<font size=\"-1\" color=\"gray\">(Remove this cell when delivering.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"#003300\">I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
